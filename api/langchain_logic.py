from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
import os

# Securely retrieve your OpenAI API key from environment variables
openai_api_key = os.environ.get('OPENAI_API_KEY')

# Initialize the OpenAI LLM with your API key and desired parameters
llm = ChatOpenAI(
    openai_api_key=openai_api_key,
    model_name='gpt-3.5-turbo',
    temperature=0.7
)

TIP_OF_TONGUE_PROMPT = """

/!TASK ::

*1. Please do not directly answer the question at first. Instead, rephrase it in full, taking care in the deft preservation of every bit of its individual levels of granularity and subtlety. Exactly matching both my levels of specificity & nuance, ensure the model preserves every minute detail. In this manner, rephrase twice. Both times with manual attention to semantic scope and the broader lexical semantics of the input. After deducing the optimal semantic space to best decode and interpret the input, inform the model; rephrase it both in a comprehensive manner in a normal vernacular and again using a more semantically dense and in a more domain exclusive tone and understood vocabulary to ensure clarity. 
*2. Building off these re-phrasings, instruct the model to create a separate markdown h2 section. This section will be a conceptual hierarchy formatted it as markdown list. Here will be a semantically dense summation of the specific thought process the model believes the user is operating in. As well as the three most likely alternate interpretations of the input string as well as the alternative intentions and thought paradigms the model suspected the user may have been operating in at the time. Within this section, the model should distinctly clarify the interpretation the model ultimately went with.
*3. Using the summation of your observations thus far, provide a expansive section facilitating some mentally stimulating and educated specious observations and broader elucidation through speculative call-and-response question answering designed to give the user the context the model believes will help guide the user towards asking the correct question.


"""

def process_message(user_query, tip_of_tongue=False):
    """
    Process the user's query with or without the Tip of the Tongue prompt.
    """
    try:
        if tip_of_tongue:
            # Step 1: Send the "Tip of the Tongue" prompt first
            tip_prompt_response = llm.invoke(TIP_OF_TONGUE_PROMPT)
            
            # Include the model's response to the prompt in the next conversation
            refined_context = tip_prompt_response.content if tip_prompt_response else ""

            # Step 2: Combine the user's query with the refined context
            conversation = f"{refined_context}\n\nUser Query: {user_query}"
        else:
            # Regular processing without the Tip of the Tongue prompt
            conversation = user_query

        # Step 3: Send the combined conversation to the model
        response = llm.invoke(conversation)
        return response.content if response else "No response generated by the model."
    except Exception as e:
        print(f"An error occurred: {e}")
        return f"Sorry, an error occurred while processing your message. {str(e)}"
